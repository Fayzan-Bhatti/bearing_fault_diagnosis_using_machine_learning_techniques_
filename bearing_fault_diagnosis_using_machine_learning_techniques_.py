# -*- coding: utf-8 -*-
"""Bearing Fault Diagnosis using Machine Learning Techniques .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wur3rTa5o7ZyXmr48HvxVD_16uP1pfz_
"""

# from google.colab import drive
# drive.mount('/content/drive')

!pip install optuna
!pip install catboost

#importing necessary libraries
import os
import scipy
import optuna
import warnings
import numpy as np
import pandas as pd
import xgboost as xgb
import catboost as cb
import lightgbm as lgbm
import scipy.stats as stats
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
warnings.filterwarnings("ignore")
from sklearn.svm import SVC
from scipy.stats import entropy
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,f1_score
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

# #Mountin the Drive
# from google.colab import drive
# drive.mount('/content/drive')

#Data paths
dataset_path_1st = '/content/drive/MyDrive/NASA baring dataset/1st_test/1st_test'
dataset_path_2nd = '/content/drive/MyDrive/NASA baring dataset/2nd_test/2nd_test'
dataset_path_3rd = '/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt'

# File path for the dataset
dataset_path = '/content/drive/MyDrive/NASA baring dataset/1st_test/1st_test/2003.10.22.12.39.13'

# Read the dataset
dataset = pd.read_csv(dataset_path, sep='\t')

# Define colors for the plots
colors = ['red', 'green', 'blue', 'orange']
# Create the plot
fig, ax = plt.subplots(figsize=(30, 10))
ax.set_title("Vibration in Bearings")
ax.set_xlabel("cycle(n)")
ax.set_ylabel("vibration/acceleration(g)")

# Plot the dataset
for i, column in enumerate(dataset.columns):
    color = colors[i % len(colors)]  # Use modulo operator to cycle through colors
    ax.plot(dataset[column], color=color, label=column)

ax.legend()  # Show legend
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Check the number of columns and adjust colors accordingly
num_columns = len(dataset.columns)
colors = ['red', 'green', 'blue', 'orange'][:num_columns]

# Scatter plot
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.scatter(range(len(dataset[column])), dataset[column], color=colors[i], label=column)
ax.set_title("Scatter Plot")
ax.set_xlabel("Index")
ax.set_ylabel("vibration/acceleration(g)")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')
# Histogram
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.hist(dataset[column], bins=20, color=colors[i], alpha=0.5, label=column)
ax.set_title("Histogram")
ax.set_xlabel("vibration/acceleration(g)")
ax.set_ylabel("Frequency")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')
# Box plot
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot(dataset.values, labels=dataset.columns, patch_artist=True, boxprops=dict(facecolor='lightgray'))
ax.set_title("Box Plot")
ax.set_ylabel("vibration/acceleration(g)")
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/2nd_test/2nd_test/2004.02.12.12.52.39', sep='\t')

# Define colors for different plots
colors = ['red', 'green', 'blue', 'orange']

# Plot the vibration graph
fig, ax = plt.subplots(figsize=(30, 10))
for i, column in enumerate(dataset.columns):
    ax.plot(dataset[column], color=colors[i], label=column)

# Set the graph title, x-axis label, and y-axis label
ax.set_title("Vibration in Bearings")
ax.set_xlabel("cycle(n)")
ax.set_ylabel("vibration/acceleration(g)")

# Display the legend
ax.legend()

# Show the plot
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/2nd_test/2nd_test/2004.02.12.12.52.39', sep='\t')

# Check the number of columns and adjust colors accordingly
num_columns = len(dataset.columns)
colors = ['red', 'green', 'blue', 'orange'][:num_columns]

# Scatter plot
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.scatter(range(len(dataset[column])), dataset[column], color=colors[i], label=column)
ax.set_title("Scatter Plot")
ax.set_xlabel("Index")
ax.set_ylabel("vibration/acceleration(g)")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/2nd_test/2nd_test/2004.02.12.12.52.39', sep='\t')
# Histogram
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.hist(dataset[column], bins=20, color=colors[i], alpha=0.5, label=column)
ax.set_title("Histogram")
ax.set_xlabel("vibration/acceleration(g)")
ax.set_ylabel("Frequency")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/2nd_test/2nd_test/2004.02.12.12.52.39', sep='\t')
# Box plot
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot(dataset.values, labels=dataset.columns, patch_artist=True, boxprops=dict(facecolor='lightgray'))
ax.set_title("Box Plot")
ax.set_ylabel("vibration/acceleration(g)")
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Define colors for each line in the graph
colors = ['red', 'green', 'blue', 'orange']

# Create the plot
fig, ax = plt.subplots(figsize=(30, 10))

# Plot each column of the dataset with the specified color
for i, column in enumerate(dataset.columns):
    ax.plot(dataset[column], color=colors[i], label=column)

# Set the plot title and axis labels
ax.set_title("Vibration in Bearings")
ax.set_xlabel("cycle(n)")
ax.set_ylabel("vibration/acceleration(g)")

# Display the legend
ax.legend()

# Show the plot
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Check the number of columns and adjust colors accordingly
num_columns = len(dataset.columns)
colors = ['red', 'green', 'blue', 'orange'][:num_columns]

# Scatter plot
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.scatter(range(len(dataset[column])), dataset[column], color=colors[i], label=column)
ax.set_title("Scatter Plot")
ax.set_xlabel("Index")
ax.set_ylabel("vibration/acceleration(g)")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')
# Histogram
fig, ax = plt.subplots(figsize=(10, 6))
for i, column in enumerate(dataset.columns):
    ax.hist(dataset[column], bins=20, color=colors[i], alpha=0.5, label=column)
ax.set_title("Histogram")
ax.set_xlabel("vibration/acceleration(g)")
ax.set_ylabel("Frequency")
ax.legend()
plt.show()

# Read the dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')
# Box plot
fig, ax = plt.subplots(figsize=(10, 6))
ax.boxplot(dataset.values, labels=dataset.columns, patch_artist=True, boxprops=dict(facecolor='lightgray'))
ax.set_title("Box Plot")
ax.set_ylabel("vibration/acceleration(g)")
plt.show()

# Root Mean Squared
def find_rms(df):
    result = []
    for col in df:
        r = np.sqrt((df[col]**2).sum() / len(df[col]))
        result.append(r)
    return result

# 1st_test dataset test to the first file
df = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Calculate RMS for each column
rms_values = find_rms(df)

# Display the calculated RMS values
for col, rms in zip(df.columns, rms_values):
    print(f"Root Mean Square: '{col}': {rms}")

# Extract peak-to-peak features
def find_p2p(df):
    return np.array(df.max().abs() + df.min().abs())

# Test for the first file for 1st_test dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Calculate peak-to-peak features for each column
p2p_values = find_p2p(dataset)

# Display the calculated peak-to-peak features
for col, p2p in zip(dataset.columns, p2p_values):
    print(f"Peak-to-Peak:'{col}': {p2p}")

# Extract Shannon entropy (cut signals to 500 bins)
def find_entropy(df):
    ent = []
    for col in df:
        ent.append(stats.entropy(pd.cut(df[col], 500).value_counts()))
    return np.array(ent)

# Test for the first file for 1st_test dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Calculate entropy for each column
entropy_values = find_entropy(dataset)

# Display the calculated entropy values
for col, ent in zip(dataset.columns, entropy_values):
    print(f"Entropy: '{col}': {ent}")

# Extract clearence factor
def find_clearence(df):
    result = []
    for col in df:
        r = ((np.sqrt(df[col].abs())).sum() / len(df[col]))**2
        result.append(r)
    return result

# Test for the first file for 1st_test dataset
dataset = pd.read_csv('/content/drive/MyDrive/NASA baring dataset/3rd_test/4th_test/txt/2004.03.07.22.12.46', sep='\t')

# Calculate clearence factor for each column
clearence_values = find_clearence(dataset)

# Display the calculated clearence factor
for col, clearence in zip(dataset.columns, clearence_values):
    print(f"Clearence factor: '{col}': {clearence}")

def time_features(dataset_path, id_set=None):
    time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']
    cols1 = ['B1_x','B1_y','B2_x','B2_y','B3_x','B3_y','B4_x','B4_y']
    cols2 = ['B1','B2','B3','B4']

    # initialize
    if id_set == 1:
        columns = [c+'_'+tf for c in cols1 for tf in time_features]
        data = pd.DataFrame(columns=columns)
    else:
        columns = [c+'_'+tf for c in cols2 for tf in time_features]
        data = pd.DataFrame(columns=columns)



    for filename in os.listdir(dataset_path):
        # read dataset
        raw_data = pd.read_csv(os.path.join(dataset_path, filename), sep='\t')

        # time features
        mean_abs = np.array(raw_data.abs().mean())
        std = np.array(raw_data.std())
        skew = np.array(raw_data.skew())
        kurtosis = np.array(raw_data.kurtosis())
        entropy = find_entropy(raw_data)
        rms = np.array(find_rms(raw_data))
        max_abs = np.array(raw_data.abs().max())
        p2p = find_p2p(raw_data)
        crest = max_abs/rms
        clearence = np.array(find_clearence(raw_data))
        shape = rms / mean_abs
        impulse = max_abs / mean_abs

        if id_set == 1:
            mean_abs = pd.DataFrame(mean_abs.reshape(1,8), columns=[c+'_mean' for c in cols1])
            std = pd.DataFrame(std.reshape(1,8), columns=[c+'_std' for c in cols1])
            skew = pd.DataFrame(skew.reshape(1,8), columns=[c+'_skew' for c in cols1])
            kurtosis = pd.DataFrame(kurtosis.reshape(1,8), columns=[c+'_kurtosis' for c in cols1])
            entropy = pd.DataFrame(entropy.reshape(1,8), columns=[c+'_entropy' for c in cols1])
            rms = pd.DataFrame(rms.reshape(1,8), columns=[c+'_rms' for c in cols1])
            max_abs = pd.DataFrame(max_abs.reshape(1,8), columns=[c+'_max' for c in cols1])
            p2p = pd.DataFrame(p2p.reshape(1,8), columns=[c+'_p2p' for c in cols1])
            crest = pd.DataFrame(crest.reshape(1,8), columns=[c+'_crest' for c in cols1])
            clearence = pd.DataFrame(clearence.reshape(1,8), columns=[c+'_clearence' for c in cols1])
            shape = pd.DataFrame(shape.reshape(1,8), columns=[c+'_shape' for c in cols1])
            impulse = pd.DataFrame(impulse.reshape(1,8), columns=[c+'_impulse' for c in cols1])

        else:
            mean_abs = pd.DataFrame(mean_abs.reshape(1,4), columns=[c+'_mean' for c in cols2])
            std = pd.DataFrame(std.reshape(1,4), columns=[c+'_std' for c in cols2])
            skew = pd.DataFrame(skew.reshape(1,4), columns=[c+'_skew' for c in cols2])
            kurtosis = pd.DataFrame(kurtosis.reshape(1,4), columns=[c+'_kurtosis' for c in cols2])
            entropy = pd.DataFrame(entropy.reshape(1,4), columns=[c+'_entropy' for c in cols2])
            rms = pd.DataFrame(rms.reshape(1,4), columns=[c+'_rms' for c in cols2])
            max_abs = pd.DataFrame(max_abs.reshape(1,4), columns=[c+'_max' for c in cols2])
            p2p = pd.DataFrame(p2p.reshape(1,4), columns=[c+'_p2p' for c in cols2])
            crest = pd.DataFrame(crest.reshape(1,4), columns=[c+'_crest' for c in cols2])
            clearence = pd.DataFrame(clearence.reshape(1,4), columns=[c+'_clearence' for c in cols2])
            shape = pd.DataFrame(shape.reshape(1,4), columns=[c+'_shape' for c in cols2])
            impulse = pd.DataFrame(impulse.reshape(1,4), columns=[c+'_impulse' for c in cols2])

        mean_abs.index = [filename]
        std.index = [filename]
        skew.index = [filename]
        kurtosis.index = [filename]
        entropy.index = [filename]
        rms.index = [filename]
        max_abs.index = [filename]
        p2p.index = [filename]
        crest.index = [filename]
        clearence.index = [filename]
        shape.index = [filename]
        impulse.index = [filename]

        # concat
        merge = pd.concat([mean_abs, std, skew, kurtosis, entropy, rms, max_abs, p2p,crest,clearence, shape, impulse], axis=1)
        data = data.append(merge)

    if id_set == 1:
        cols = [c+'_'+tf for c in cols1 for tf in time_features]
        data = data[cols]
    else:
        cols = [c+'_'+tf for c in cols2 for tf in time_features]
        data = data[cols]

    data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')
    data = data.sort_index()
    return data

# def time_features(dataset_path, id_set=None):
#     time_features = ['mean', 'std', 'skew', 'kurtosis', 'entropy', 'rms', 'max', 'p2p', 'crest', 'clearence', 'shape', 'impulse']
#     cols1 = ['B1_x', 'B1_y', 'B2_x', 'B2_y', 'B3_x', 'B3_y', 'B4_x', 'B4_y']
#     cols2 = ['B1', 'B2', 'B3', 'B4']

#     # initialize
#     if id_set == 1:
#         columns = [c+'_'+tf for c in cols1 for tf in time_features]
#     else:
#         columns = [c+'_'+tf for c in cols2 for tf in time_features]

#     data = pd.DataFrame(columns=columns)

#     for filename in os.listdir(dataset_path):
#         # read dataset
#         raw_data = pd.read_csv(os.path.join(dataset_path, filename), sep='\t')

#         # time features
#         features = {}

#         features['mean_abs'] = np.array(raw_data.abs().mean())
#         features['std'] = np.array(raw_data.std())
#         features['skew'] = np.array(raw_data.skew())
#         features['kurtosis'] = np.array(raw_data.kurtosis())
#         features['entropy'] = find_entropy(raw_data)
#         features['rms'] = np.array(find_rms(raw_data))
#         features['max_abs'] = np.array(raw_data.abs().max())
#         features['p2p'] = find_p2p(raw_data)

#         if 'rms' in features and 'max_abs' in features:
#             features['crest'] = features['max_abs'] / features['rms']
#         else:
#             features['crest'] = np.nan

#         features['clearence'] = np.array(find_clearence(raw_data))

#         if 'rms' in features and 'mean_abs' in features:
#             features['shape'] = features['rms'] / features['mean_abs']
#             features['impulse'] = features['max_abs'] / features['mean_abs']
#         else:
#             features['shape'] = np.nan
#             features['impulse'] = np.nan

#         print("Available features:", features.keys())  # Debugging line

#         reshaped_features = {}
#         if id_set == 1:
#             for col in cols1:
#                 for tf in time_features:
#                     if tf in features:
#                         feature_vals = features[tf][:8]
#                         if len(feature_vals) == len(raw_data):
#                             reshaped_features[col+'_'+tf] = feature_vals
#                         else:
#                             reshaped_features[col+'_'+tf] = np.append(feature_vals, [np.nan] * (len(raw_data) - len(feature_vals)))
#                     else:
#                         reshaped_features[col+'_'+tf] = np.nan
#         else:
#             for col in cols2:
#                 for tf in time_features:
#                     if tf in features:
#                         feature_vals = features[tf][:4]
#                         if len(feature_vals) == len(raw_data):
#                             reshaped_features[col+'_'+tf] = feature_vals
#                         else:
#                             reshaped_features[col+'_'+tf] = np.append(feature_vals, [np.nan] * (len(raw_data) - len(feature_vals)))
#                     else:
#                         reshaped_features[col+'_'+tf] = np.nan

#         df = pd.DataFrame(reshaped_features, index=raw_data.index)
#         data = data.append(df)

#     if id_set == 1:
#         data = data[[c+'_'+tf for c in cols1 for tf in time_features]]
#     else:
#         data = data[[c+'_'+tf for c in cols2 for tf in time_features]]

#     data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')
#     data = data.sort_index()
#     return data

myfile = time_features(dataset_path_1st, id_set=1)
myfile.to_csv('timefeature_for_myxfile.csv')

myfile.describe()

myfile.columns

class TimeFeaturesPlotter:
    def __init__(self, time_features_list, bearings_xy, myfile):
        self.time_features_list = time_features_list
        self.bearings_xy = bearings_xy
        self.myfile = myfile

    def plot_time_features(self):
        colors = ['red', 'blue', 'green', 'orange']

        for tf in self.time_features_list:
            fig = plt.figure()
            axes = []

            for i in range(2):
                ax = fig.add_subplot(141 + i)
                axes.append(ax)

                col = self.bearings_xy[0][i] + tf
                self.myfile[col].plot(figsize=(30, 10), title="Bearing{} x-y_{}".format(i + 1, tf),
                                    legend=True, ax=ax, color=colors[i])

                col = self.bearings_xy[1][i] + tf
                self.myfile[col].plot(figsize=(36, 6), legend=True, ax=ax)

                ax.set(xlabel="cycle", ylabel="value")

            plt.show()

# Usage example:
time_features_list = ["mean", "std", "skew", "kurtosis", "entropy", "rms", "max", "p2p", "crest", "clearence", "shape", "impulse"]
bearings_xy = [["B"+str(n)+"_"+str(o)+"_" for n in range(1, 5)] for o in ['x', 'y']]

plotter = TimeFeaturesPlotter(time_features_list, bearings_xy, myfile)
plotter.plot_time_features()

health_status_labels = {
    "B1": {
        "early": ["2003-10-22 12:06:24", "2003-10-23 09:14:13"],
        "suspect": ["2003-10-23 09:24:13", "2003-11-08 12:11:44"],
        "normal": ["2003-11-08 12:21:44", "2003-11-19 21:06:07"],
        "suspect_1": ["2003-11-19 21:16:07", "2003-11-24 20:47:32"],
        "imminent_failure": ["2003-11-24 20:57:32", "2003-11-25 23:39:56"]
    },
    "B2": {
        "early": ["2003-10-22 12:06:24", "2003-11-01 21:41:44"],
        "normal": ["2003-11-01 21:51:44", "2003-11-24 01:01:24"],
        "suspect": ["2003-11-24 01:11:24", "2003-11-25 10:47:32"],
        "imminent_failure": ["2003-11-25 10:57:32", "2003-11-25 23:39:56"]
    },
    "B3": {
        "early": ["2003-10-22 12:06:24", "2003-11-01 21:41:44"],
        "normal": ["2003-11-01 21:51:44", "2003-11-22 09:16:56"],
        "suspect": ["2003-11-22 09:26:56", "2003-11-25 10:47:32"],
        "Inner_race_failure": ["2003-11-25 10:57:32", "2003-11-25 23:39:56"]
    },
    "B4": {
        "early": ["2003-10-22 12:06:24", "2003-10-29 21:39:46"],
        "normal": ["2003-10-29 21:49:46", "2003-11-15 05:08:46"],
        "suspect": ["2003-11-15 05:18:46", "2003-11-18 19:12:30"],
        "Rolling_element_failure": ["2003-11-19 09:06:09", "2003-11-22 17:36:56"],
        "Stage_two_failure": ["2003-11-22 17:46:56", "2003-11-25 23:39:56"]
    }
}

# Example usage to access the health status labels for B1:
early_start, early_end = health_status_labels["B1"]["early"]

from collections import Counter

B1_state = [''] * len(myfile.index)
B2_state = [''] * len(myfile.index)
B3_state = [''] * len(myfile.index)
B4_state = [''] * len(myfile.index)

for cnt, row in enumerate(myfile.index, start=1):
    # B1
    if cnt <= 151:
        B1_state[cnt-1] = "early"
    elif 151 < cnt <= 600:
        B1_state[cnt-1] = "suspect"
    elif 600 < cnt <= 1499:
        B1_state[cnt-1] = "normal"
    elif 1499 < cnt <= 2098:
        B1_state[cnt-1] = "suspect"
    elif 2098 < cnt <= 2156:
        B1_state[cnt-1] = "imminent_failure"

    # B2
    if cnt <= 500:
        B2_state[cnt-1] = "early"
    elif 500 < cnt <= 2000:
        B2_state[cnt-1] = "normal"
    elif 2000 < cnt <= 2120:
        B2_state[cnt-1] = "suspect"
    elif 2120 < cnt <= 2156:
        B2_state[cnt-1] = "imminent_failure"

    # B3
    if cnt <= 500:
        B3_state[cnt-1] = "early"
    elif 500 < cnt <= 1790:
        B3_state[cnt-1] = "normal"
    elif 1790 < cnt <= 2120:
        B3_state[cnt-1] = "suspect"
    elif 2120 < cnt <= 2156:
        B3_state[cnt-1] = "Inner_race_failure"

    # B4
    if cnt <= 200:
        B4_state[cnt-1] = "early"
    elif 200 < cnt <= 1000:
        B4_state[cnt-1] = "normal"
    elif 1000 < cnt <= 1435:
        B4_state[cnt-1] = "suspect"
    elif 1435 < cnt <= 1840:
        B4_state[cnt-1] = "Inner_race_failure"
    elif 1840 < cnt <= 2156:
        B4_state[cnt-1] = "Stage_two_failure"

# Controlling the counts
print(Counter(B1_state))
print(Counter(B2_state))
print(Counter(B3_state))
print(Counter(B4_state))

myfile['B1_state'] = B1_state
myfile['B2_state'] = B2_state
myfile['B3_state'] = B3_state
myfile['B4_state'] = B4_state

myfile.head()

class DataProcessor:
    def __init__(self, myfile):
        self.myfile = myfile
        self.B1_cols = []
        self.B2_cols = []
        self.B3_cols = []
        self.B4_cols = []
        self.B1 = None
        self.B2 = None
        self.B3 = None
        self.B4 = None
        self.cols = ['Bx_mean','Bx_std','Bx_skew','Bx_kurtosis','Bx_entropy','Bx_rms','Bx_max','Bx_p2p','Bx_crest', 'Bx_clearence', 'Bx_shape', 'Bx_impulse',
                    'By_mean','By_std','By_skew','By_kurtosis','By_entropy','By_rms','By_max','By_p2p','By_crest', 'By_clearence', 'By_shape', 'By_impulse',
                    'class']
        self.final_data = None

    def process_data(self):
        self.B1_cols = [col for col in self.myfile.columns if "B1" in col]
        self.B2_cols = [col for col in self.myfile.columns if "B2" in col]
        self.B3_cols = [col for col in self.myfile.columns if "B3" in col]
        self.B4_cols = [col for col in self.myfile.columns if "B4" in col]

        self.B1 = self.myfile[self.B1_cols]
        self.B2 = self.myfile[self.B2_cols]
        self.B3 = self.myfile[self.B3_cols]
        self.B4 = self.myfile[self.B4_cols]

        self.B1.columns = self.cols
        self.B2.columns = self.cols
        self.B3.columns = self.cols
        self.B4.columns = self.cols

        self.final_data = pd.concat([self.B1, self.B2, self.B3, self.B4], axis=0, ignore_index=True)

    def describe_data(self):
        if self.final_data is not None:
            return self.final_data.describe()
        else:
            raise ValueError("Data has not been processed yet. Call 'process_data()' first.")


# Usage:
data_processor = DataProcessor(myfile)
data_processor.process_data()
data_description = data_processor.describe_data()
print(data_description)

X = data_description.copy()

if 'class' not in X.columns:
    print("Error: 'class' column not found in the DataFrame.")
    exit()
    print("aaa")

y = X.pop("class")
le = preprocessing.LabelEncoder()
le.fit(y)
y = le.transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)